from fastapi import FastAPI, UploadFile, File, Form, Depends, HTTPException
from fastapi.responses import RedirectResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from pydantic import BaseModel
from langserve import add_routes
from chat import chain as chat_chain
from dotenv import load_dotenv
from typing import List, Optional
from PIL import Image
import json
import pytesseract
import io
import os
import requests
import uuid
import logging
import datetime
import asyncio
from apscheduler.schedulers.background import BackgroundScheduler
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain.chains.retrieval_qa.base import RetrievalQA
from es_utils import search_documents_es, save_to_elasticsearch
from chat import llm
from data import fetch_data_prec, fetch_data_law, fetch_data_ordin
from rag import save_files_to_vector_db, get_vector_db

# í™˜ê²½ ì„¤ì • íŒŒì¼ ë¡œë”©
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama_container:11434")
ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST", "http://es_container:9200")

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°ì²´ ì´ˆê¸°í™”
app = FastAPI()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

logger.info("ğŸš€ ì„œë²„ ì‹œì‘ ì¤‘...")

# Elasticsearch ë° Ollama ì—°ê²° í™•ì¸
def check_services():
    """Elasticsearch & Ollama ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        es_response = requests.get(f"{ELASTICSEARCH_HOST}/_cluster/health", timeout=5)
        if es_response.status_code == 200:
            logger.info("âœ… Elasticsearch ì—°ê²° ì„±ê³µ!")
        else:
            logger.warning("âš ï¸ Elasticsearch ì‘ë‹µ ì˜¤ë¥˜: ìƒíƒœ ì½”ë“œ %d", es_response.status_code)
    except requests.exceptions.RequestException as e:
        logger.error("âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨: %s", str(e))

    try:
        ollama_response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if ollama_response.status_code == 200:
            logger.info("âœ… Ollama ì—°ê²° ì„±ê³µ!")
        else:
            logger.warning("âš ï¸ Ollama ì‘ë‹µ ì˜¤ë¥˜: ìƒíƒœ ì½”ë“œ %d", ollama_response.status_code)
    except requests.exceptions.RequestException as e:
        logger.error("âŒ Ollama ì—°ê²° ì‹¤íŒ¨: %s", str(e))


# Tesseract OCR ê²½ë¡œ ì„¤ì •
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

# ë²¡í„° DB ì„¤ì •
CHROMA_DB_DIR = "vectorstore"
PDF_DIR = "pdfs"
embeddings = OllamaEmbeddings(model="llama3.2")

# ê¸°ì¡´ DB ë””ë ‰í† ë¦¬ ì‚­ì œ
if os.path.exists(CHROMA_DB_DIR):
   import shutil
   shutil.rmtree(CHROMA_DB_DIR)  # ë””ë ‰í† ë¦¬ ë° ê·¸ ì•ˆì˜ ë‚´ìš© ëª¨ë‘ ì‚­ì œ
    
os.makedirs(CHROMA_DB_DIR, exist_ok=True)

# ë²¡í„° DB ìƒì„± ë° retriever ì„¤ì •
vector_db = get_vector_db()
retriever = vector_db.as_retriever()

qa_chain = RetrievalQA.from_chain_type(
    llm=llm, chain_type="map_reduce", retriever=retriever, return_source_documents=True
)

# ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
scheduler = BackgroundScheduler()


def fetch_all_data():
    """ë²•ë ¹ ë°ì´í„° ìˆ˜ì§‘ & ë²¡í„° DB ì €ì¥"""
    fetch_data_prec()
    fetch_data_law()
    fetch_data_ordin()
    
     # fetch_all_dataê°€ ì™„ë£Œëœ í›„ì— save_files_to_vector_db í˜¸ì¶œ
    save_files_to_vector_db()

# êµ­ê°€ë²•ë ¹ì •ë³´ ì—‘ì…€ ì—…ë°ì´íŠ¸
scheduler.add_job(fetch_all_data, trigger='interval', hours=24)


# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # ë°°í¬ì‹œ ë„ë©”ì¸
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)


# Ollama API í˜¸ì¶œ í•¨ìˆ˜
def query_ollama(messages: list[str]):
    """Ollama APIì— ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ëŠ” í•¨ìˆ˜"""
    url = f"{OLLAMA_BASE_URL}/api/generate"
    payload = {
        "model": "llama3.2",
        "prompt": "\n".join(messages),
        "stream": False
    }
    
    try:
        response = requests.post(url, json=payload)
        logger.error(f"response_dataê°’ í™•ì¸: {response.text}")
        
        response.raise_for_status()
        
        response_data = response.json()
        

        # ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ì¸ì§€ í™•ì¸ í›„ ì²˜ë¦¬
        if not isinstance(response_data, dict):
            logger.error(f"âŒ Ollama ì‘ë‹µì´ ì˜ˆìƒí•œ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {response_data}")
            response_data = {"response": str(response_data)}

        chatbot_response = response_data.get("response", "âš ï¸ Ollama ì‘ë‹µ ì—†ìŒ")
        return chatbot_response

    except requests.exceptions.Timeout:
        logger.error("âŒ Ollama API ìš”ì²­ ì‹œê°„ ì´ˆê³¼!")
        return "â›” Ollama ì‘ë‹µì´ ì§€ì—°ë˜ì—ˆìŠµë‹ˆë‹¤."

    except requests.exceptions.ConnectionError:
        logger.error("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return "â›” Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Ollama API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return f"â›” Ollama API ì˜¤ë¥˜: {e}"



# ê¸°ë³¸ ê²½ë¡œ("/")
@app.get("/")
async def redirect_root_to_docs():
    return RedirectResponse("/prompt/playground")

@app.get("/prompt/playground")
async def playground():
    return {"message": "Welcome to the Playground!"}

########### ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ ###########

# ì…ë ¥ ë°ì´í„° ëª¨ë¸
class InputChat(BaseModel):
    messages: list[str]

# ëŒ€í™”í˜• API ì—”ë“œí¬ì¸íŠ¸
@app.post("/chat")
async def chat(input: str = Form(...), file: Optional[UploadFile] = File(None),  document_type: str = "message", vector_db: Chroma = Depends(get_vector_db)):
    # vector_dbëŠ” ì´ì œ Chroma ê°ì²´ë¡œ ìë™ ì£¼ì…ë¨
    try:
        input_data = InputChat(**json.loads(input))
        result = {}

        # ëŒ€í™” ID ìƒì„±
        conversation_id = str(uuid.uuid4())

        # OCR ì´ë¯¸ì§€ ì²˜ë¦¬ (ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹œ OCR í…ìŠ¤íŠ¸ ì²˜ë¦¬)
        if file:
            image = Image.open(io.BytesIO(await file.read()))
            ocr_text = pytesseract.image_to_string(image, lang="kor+eng")
            input_data.messages.append(ocr_text)
            result["ocr_text"] = ocr_text
            
        
        # # ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        # query = input_data.messages[-1]  # ìµœì‹  ë©”ì‹œì§€ ì‚¬ìš©
        # docs = retriever.invoke(query)

        # # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLM ì…ë ¥ì— ì¶”ê°€
        # context = "\n\n".join([doc.page_content for doc in docs]) if docs else "ê´€ë ¨ ì •ë³´ ì—†ìŒ"
        # input_data.messages.append(f"ğŸ” ì°¸ê³  ì •ë³´:\n{context}")

        # # ì±—ë´‡ ì‘ë‹µ ìƒì„±
        # result["chatbot_response"] = chat_chain.invoke(input_data.messages)    
        
        # ì±—ë´‡ ì‘ë‹µë§Œ ìƒì„± í”„ë¡¬í”„íŒ… Test
        # result["chatbot_response"] = chat_chain.invoke(input_data.messages)


        # ES & Chroma ê²€ìƒ‰ (ë¹„ë™ê¸° ì²˜ë¦¬ ê°œì„ )
        query = input_data.messages[-1]
        logger.info("ğŸ” ES & Chroma ê²€ìƒ‰ ì‹œì‘")

        try:
            # ë¹„ë™ê¸° ì‘ì—… ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ë¬¸ì œ ë°œìƒ ì‹œ ê° ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•˜ë‚˜ì”© ì²˜ë¦¬)
            chroma_results = await asyncio.to_thread(retriever.invoke, query)
            logger.info(f"{chroma_results}")
            es_results = await asyncio.to_thread(search_documents_es, query)
            logger.info(f"{es_results}")

            logger.info("âœ… ES & Chroma ê²€ìƒ‰ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ES & Chroma ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            chroma_results, es_results = [], []

        # ê²€ìƒ‰ ê²°ê³¼ í•©ì¹¨
        chroma_texts = [doc.page_content for doc in chroma_results]
        es_texts = [doc["text"] for doc in es_results]
        docs = chroma_texts + es_texts
        context = "\n\n".join(docs) if docs else "ê´€ë ¨ ì •ë³´ ì—†ìŒ"
        input_data.messages.append(f"ğŸ” ì°¸ê³  ì •ë³´:\n{context}")

        # Ollama API í˜¸ì¶œ
        logger.info("ğŸŸ¢ Ollama API í˜¸ì¶œ ì‹œì‘")
        chatbot_response = query_ollama(input_data.messages)
        logger.info(f"ğŸŸ¢ Ollama ì‘ë‹µ: {chatbot_response}")

        result["chatbot_response"] = chatbot_response
        
        # ë²¡í„° DBì— ë©”ì‹œì§€ ì €ì¥
        # save_to_vector_db(input_data.messages, document_type, conversation_id, vector_db)

        return JSONResponse(content={"message": "Chat response", "type": document_type, "result": result}, headers={"Content-Type": "application/json; charset=UTF-8"})
        # return JSONResponse(content={"message": "Chat response", "type": document_type, "result": result, "input_messages": input_data.messages})
    
    except Exception as e:
        logger.error(f"âŒ ì±—ë´‡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


# ëŒ€í™”í˜• ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
add_routes(
    app,
    chat_chain.with_types(input_type=InputChat),
    path="/chat",
    enable_feedback_endpoint=True,
    enable_public_trace_link_endpoint=True,
    playground_type="chat",
)

# ì„œë²„ ì‹¤í–‰ ì„¤ì •
if __name__ == "__main__":
    scheduler.start()
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ ì‹¤í–‰
    # fetch_all_data()
    
    check_services()  # Ollama & Elasticsearch ì—°ê²° í…ŒìŠ¤íŠ¸
    uvicorn.run(app, host="0.0.0.0", port=8000)
